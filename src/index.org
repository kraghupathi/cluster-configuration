#+TITLE:     The Cluster Model
#+AUTHOR:    M.S.Soumya
#+EMAIL:     ms@ms
#+DATE:      2015-03-25 Wed
#+PROPERTY: session *scratch*
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org

* Introduction
  This document describes in detail the model of the systems cluster
  at VLEAD.  The tools, systems, configurations, setup files and
  related discussions are all part of the model.
  
* The Cluster Model
  The cluster model describes the set of systems which together
  provide the infrastructure for the hosting of the virtual-labs.
  These virtual-labs are deployed at the following different
  locations:
  + [[Base4 Cluster][Base4]]
  + [[Base1 Cluster][Base1]]
  + [[Amazon Web Services (AWS) Cluster][AWS]]
  using the same model with the minor differences in the provisioning.
  The minor differences in provisioning are highlighted later in this
  model.
  
* Bootstrapping Cluster
  A independent operating system such as a container or a VM or even a
# comment 'An independent' instead of 'a independent'
  physical machine is referred as a node.  The collection of such
  related and connected nodes forms a cluster.  Each node in the
  cluster has a specific purpose that it serves.  To setup an entire
  cluster initially, each of the component nodes are setup
  individually in an planned and specific manner.  This process of
# comment 'in a planned' instead of 'in an planned'
  setup of individual nodes in a specific manner to setup a new
  cluster is called the bootstrapping process.  This process is
  tightly related to the provisioning method used on the corresponding
  cluster.

  Following provisioning methods have been used on the individual
  clusters:
  - AWS cluster :: AWS cluster is created using VMs as nodes.  These
                   VMs are provisioned using AWS web console, AWS CLI
                   tools or AWS python API
  - Base1 cluster :: Base1 cluster is created using OpenVZ containers
                     which use bridged networking with ethernet
                     interfaces (=--netif_add=).  These containers do
                     not use venet interfaces (=--ipadd=) as venet
                     interfaces do not allow routing to be changed.
                     In base1 cluster we need nodes to have
                     configurable routing so that we can set a custom
                     gateway.  
  - Base4 cluster :: Same as base1
  
* Purpose of multiple deployments
** Base4 Cluster
   This cluster is a development cluster.  This cluster is used to
   test new changes made to the develop branch before they are
   considered as stable and merged with master.

** Base1 Cluster 
   Master branches which are found to be stable on the base4 cluster
   are released with a tag and version number.  The new release is
   then tested on base1 cluster to re-verify its soundness on staging
   environment, before it is pushed to production environment on AWS.
   
** Amazon Web Services (AWS) Cluster
   This cluster is the production cluster.  This cluster is modified
   after changes have been verified on both development (base4) and
   staging (base1).

   /It should be noted that ADS adapter used on base1 and base4 is different than adapter used on AWS.  Hence some problems may arise due to use of different adapter in production in comparison to staging and development./ 

   This is the main cluster used by the external virtual-labs users
   across the nation.  

<<<<<<< HEAD
# comment the above statement is not clear
* Bootstrapping of a cluster deployment
  The cluster is a collection of several nodes.  Each of these nodes
  have a specific purpose that they serve.  To setup the entire
  cluster for the first time we need to setup each of the nodes
  individually.  This process is called the bootstrapping process.
  This process is tightly related to provisioning on corresponding
  cluster. 
** Provisioning vs bootstrapping
   *Provisioning* is a term used to describe the process for bringing up
   a system for setting up the node.\\
   *Bootstrapping* process uses this system to setup the
   nodes. Provisioning is a part of the bootstrapping process, hence
   they cannot be separated.\\
** Bootstrapping of Cluster
   All the nodes of the three deployments are same.  The main
   difference is in the provisioning.  The details of provisioning of
   these clusters is described in the following sections.

*** Provisioning for Base1 and Base4
    The 
# comment Above statement is incomplete
*** Provisioning for AWS
# comment This section needs to be filled    
* Design of the Cluster model
  The diagram below depicts the architecture of the systems in the
# commment Reference is made to a diagram that does not exist
  cluster model. This architecture does not change for the types of
  clusters mentioned earlier.  
* Difference in the base1 and AWS cluster
  Most of the components of base1 and base4 clusters are same as AWS
  cluster.  The main difference is in the provisioning.  The details of
  provisioning of these clusters is described in this section.

** Provisioning of the base1 and base4 clusters
   *TODO*
** Provisioning of the AWS cluster
   *TODO*

* Design of the cluster model
  The diagram below depicts the architecture of the nodes in the
  cluster model.  This architecture is same for all the three
  deployments (base1, base4 and AWS).
  
** Overall Network Diagram
   The below diagram shows the network setup of the cluster and
   connectivity between few key nodes.

   #+CAPTION:  Cluster Network Diagram
   #+LABEL:  fig-cluster-network-diagram
   #+NAME: fig-cluster-network-diagram
   [[./diagrams/overall-cluster-network-diagram.png]]
   
   In this diagram IP1 and IP2 refer to the only two public IPs being
   used by the system.  IP1 is the public IP assigned to router and
   IP2 is the public IP assigned to ansible.  Normal virtual-lab users
   only see and contact router.  VLEAD team uses ansible to manage the
   cluster.  In near future ADS interface will also be accessible via
   router, for all virtual-labs developers, perhaps through a
   different port. \\
   This diagram focuses on depicting the important connections in the
   network. There are several other connections which are not shown.
   For example, ansible is connected to every node in the cluster.
   All the connections are not shown in the diagram. Also there are
   some nodes which are not shown, for example the rsnapshot server,
   rsyslog server, ossec server are not shown. But these nodes are
   configured and are being used in the cluster but are not shown to
   keep the diagram simple.

* Nodes in the cluster
  After the bootstrapping process the nodes in the cluster can be
  brought up using the scripts below. 
** Structure of the scripts
  The tree structure of scripts which are used to configure the
  cluster is as shown below. The structure shows all the roles which
  are the nodes. The 
# comment Above statement is incomplete
#+BEGIN_EXAMPLE
|-- ads_server.yaml
|-- ansible_server.yaml
|-- documentation
|   |-- ads-setup-on-base1.org
|   |-- base1-setup-notes.org
|   `-- KNOWN_ISSUES.org
|-- hosts
|   |-- base1
|   `-- host_only
|-- lab_rnapshot_server.yaml
|-- nagios_server.yaml
|-- ossec_server.yaml
|-- private_dns.yaml
|-- public_dns.yaml
|-- reverseproxy_server.yaml
|-- router.yaml
|-- rsnapshot_server.yaml
|-- rsyslog_server.yaml
|-- site.yaml
|-- roles
|   |-- ads_server
|   |   |-- files
|   |   |   `-- run_setup.sh
|   |   |-- handlers
|   |   |   `-- main.yml
|   |   |-- tasks
|   |   |   `-- main.yml
|   |   |-- templates
|   |   |   |-- iptables
|   |   |   `-- mongodb.repo
|   |   `-- vars
|   |       `-- main.yml
|   |-- ansible_server
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- templates
|   |       `-- ansible_iptables
|   |-- common
|   |   |-- files
|   |   |   `-- history.sh
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- meta
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   |-- templates
|   |   |   `-- resolv.conf
|   |   `-- vars
|   |       `-- main.yaml
|   |-- common_vars
|   |   `-- vars
|   |       `-- main.yaml
|   |-- glpi_server
|   |   |-- files
|   |   |   `-- index.html
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- meta
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- vars
|   |       `-- main.yaml
|   |-- lab_rsnapshot_server
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   |-- templates
|   |   |   |-- rsnapshot.conf.j2
|   |   |   `-- rsnapshot_server_iptables
|   |   `-- vars
|   |       `-- main.yaml
|   |-- nagios_client
|   |   |-- defaults
|   |   |   `-- main.yaml
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- inprogress.txt
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- templates
|   |       `-- xinetd.conf.j2
|   |-- nagios_server
|   |   |-- defaults
|   |   |   `-- main.yaml
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   |-- configure_servers.yaml
|   |   |   `-- main.yaml
|   |   |-- templates
|   |   |   |-- commands.cfg
|   |   |   |-- contacts.cfg.j2
|   |   |   `-- servers.cfg
|   |   `-- vars
|   |       `-- main.yaml
|   |-- named_server
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- templates
|   |       |-- named.conf
|   |       |-- named_iptables
|   |       `-- zone.forward
|   |-- ossec_client
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   |-- templates
|   |   |   `-- ossec_client_input.j2
|   |   `-- vars
|   |       `-- main.yaml
|   |-- ossec_server
|   |   |-- files
|   |   |   |-- add_agent.sh
|   |   |   |-- index.html
|   |   |   `-- ossec_webui_setup.sh
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   |-- get_client_keys.yaml
|   |   |   |-- main.yaml
|   |   |   |-- setup_ossec_server.yaml
|   |   |   `-- setup_webui.yaml
|   |   |-- templates
|   |   |   |-- ossec_iptables
|   |   |   `-- ossec_server_input.j2
|   |   `-- vars
|   |       `-- main.yaml
|   |-- redmine_server
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- meta
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- vars
|   |       `-- main.yaml
|   |-- reverseproxy_server
|   |   |-- files
|   |   |   `-- httpd_awstats.conf
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- templates
|   |       |-- awstats.reverseproxy.conf
|   |       |-- reverseproxy_iptables
|   |       `-- virtualhost.conf
|   |-- router
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- templates
|   |       `-- router_iptables
|   |-- rsnapshot_client
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- vars
|   |       `-- main.yaml
|   |-- rsnapshot_server
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   |-- templates
|   |   |   |-- rsnapshot.conf.j2
|   |   |   `-- rsnapshot_server_iptables
|   |   `-- vars
|   |       `-- main.yaml
|   |-- rsyslog_client
|   |   |-- handlers
|   |   |   `-- main.yaml
|   |   |-- tasks
|   |   |   `-- main.yaml
|   |   `-- templates
|   |       `-- rsyslog_client.conf
|   `-- rsyslog_server
|       |-- files
|       |   `-- rsyslog_server.conf
|       |-- handlers
|       |   `-- main.yaml
|       |-- tasks
|       |   `-- main.yaml
|       `-- templates
|           `-- rsyslog_iptables
|-- rsnapshot_server_pubic_key
|   `-- 10.100.1.10
|       `-- root
`-- run.sh
#+END_EXAMPLE

** Scripts 
  These are the Ansible scripts which are written in the form of
  roles.
  #+BEGIN_SRC 
  ---
  #Complete site configuration file
  #One yaml file for each server is included here
  #Server yaml file matches server FQDN

  - include: ansible_server.yaml

  - include: ossec_server.yaml

  - include: public_dns.yaml

  - include: private_dns.yaml

  - include: rsyslog_server.yaml

  - include: reverseproxy_server.yaml

  - include: router.yaml

  - include: nagios_server.yaml

  #- include: ads_server.yaml

  - include: rsnapshot_server.yaml
  #+END_SRC 
  There are several nodes in the cluster which are common to
  the various deployments. The list of nodes is as below:
- [[./rsyslog_server.org][Rsyslog]]
=======
   used by the system.  IP1 is the public IP assigned to the router
   and IP2 is the public IP assigned to the ansible.  Normal
   virtual-lab users only see and contact router.  VLEAD team uses the
   ansible node to manage the cluster.  

   /In near future an ADS interface will also be accessible via router, for all the virtual-labs developers, perhaps through a different port./

** Nodes in the cluster
- [[./rsyslog_server.org][Rsyslog server]]
- [[./privatedns.org][Private DNS]]
- [[./publicdns.org][Public DNS]]
- [[./rp-awstats.org][Reverse Proxy]]
- [[./nagios_server.org][Nagios server]]
- [[./router.org][Router]]
- [[./rsnapshot_server.org][Backup server]]
- [[./ossec_server.org][OSSEC server]] *TODO*

