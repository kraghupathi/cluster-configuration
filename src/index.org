#+TITLE:     The Cluster Model
#+DATE:      2015-03-25 Wed
#+PROPERTY: session *scratch*
#+PROPERTY: results output
#+PROPERTY: exports code
#+SETUPFILE: org-templates/level-0.org
#+OPTIONS: ^nil

* Introduction
  This document describes in detail the model of the systems cluster
  at VLEAD.  The tools, systems, configurations, setup files and
  related discussions are all part of the model.
 
* Prerequisites
  This section enlists the basic knowledge the reader must have to be
  able to comprehend the details of this model. The reader must be
  familiar with the following:
  + Networking basics
  + Ansible 
  + Amazon Web Service (AWS) [EC2, VM creation, console, AMI etc.]
  + OpenVZ 
  + Version control basics
To know further about these please refer [[Appendix][Appendix]].

* The Cluster Model
  The cluster model describes the set of systems which together
  provide the infrastructure for the hosting of the virtual-labs.
  This infrastructure is setup at the following different locations:
  + [[Base4 Cluster][Base4]]
  + [[Base1 Cluster][Base1]]
  + [[Amazon Web Services (AWS) Cluster][AWS]]
  The names of the cluster are given based on the locations in which
  they are deployed. Base1 and Base4 are Centos machines running
  OpenVZ kernel and are physically located in IIIT. 

* Purpose of multiple clusters
** Base4 Cluster
   This cluster is a development cluster.  This cluster is used to
   test new changes made to the develop branch before they are
   considered as stable and merged with the master branch.

** Base1 Cluster 
   Stable commits to the master branch on the base4 cluster are
   released.  Each release has a tag and a version number.  A new
   release is tested on base1 cluster to re-verify its soundness on
   the staging environment, before it is pushed to the production
   environment on AWS.

** Amazon Web Services (AWS) Cluster
   This cluster is the production cluster.  This cluster is modified
   only after changes have been verified on both development (base4)
   and staging (base1) clusters.

   /It should be noted that ADS adapter used on base1 and base4 is/
   /different than adapter used on AWS.  Hence some problems may arise/
   /due to use of different adapter in production in comparison to/
   /staging and development./

   This is the main cluster used by the external virtual-labs users
   across the nation.  

* Bootstrapping of a cluster
  An independent operating system such as a container or a VM or even
  a physical machine is referred as a node.  The collection of such
  related and connected nodes forms a cluster.  Each node in the
  cluster has a specific purpose that it serves.  To setup an entire
  cluster initially, each of the component nodes are setup
  individually in a planned and specific manner.  This process of
  setup of individual nodes in a specific manner to setup a new
  cluster is called the bootstrapping process.  This process is
  tightly related to the provisioning method used on the corresponding
  cluster.
** Provisioning 
  Following provisioning methods have been used on the individual
  clusters:
  - AWS cluster :: AWS cluster is created using VMs as nodes.  These
                   VMs are provisioned using AWS web console, AWS CLI
                   tools or AWS python API
  - Base1 cluster :: Base1 cluster is created using OpenVZ containers
                     which use bridged networking with ethernet
                     interfaces (=--netif_add=).  These containers do
                     not use venet interfaces (=--ipadd=) as venet
                     interfaces do not allow routing to be changed.
                     In base1 cluster we need nodes to have
                     configurable routing so that we can set a custom
                     gateway.  
  - Base4 cluster :: Same as base1

** Bootstrapping the AWS cluster
*** Ansible machine creation
    Create a machine for ansible installation.  Setup rpmfusion-free,
    rpmfusion-nonfree, rpmforge and epel repos on the system.  Use
    #+BEGIN_EXAMPLE
    yum -y install ansible
    #+END_EXAMPLE
    to install ansible. 
     Create a 'vlead' user in the machine.
     #+BEGIN_EXAMPLE
     adduser vlead 
     #+END_EXAMPLE
     ssh public key of this user has to be copied in all other machine.  If the
     root user has already been created then the root user's private and public
     key can be placed inside the vlead user's .ssh directory, so that from
     vlead user we can login to other machine roots account.

*** Version control setup
    Setup version control on ansible machine using:
    #+BEGIN_EXAMPLE
    yum -y install bzr
    #+END_EXAMPLE

*** Setup basic ansible server
    Setup trust based ssh from ansible_server to itself.  Edit or
    create hosts file to point to correct ansible server.  Ensure that
    run.sh refers to correct hosts file.  
    
    Ensure that roles/common_vars/vars/main.yaml has:
    1. Correct ansible_server_ips.
    2. Private_dns_ips and private_dns_zone are set to none.  

    Use only roles common and ansible to setup ansible server
    directly using ansible.yaml.

*** Setup ossec server
    Create ossec server and setup trust based SSH to it.  Edit hosts
    file to reflect correct IP of ossec-server.  Ensure that
    roles/common_vars/vars/main.yaml has correct ossec_server_ip and
    ossec_client_ips.  Ossec server should be setup with common and
    ossec-server roles only.

*** Enable ossec-client role on ansible server
    Enable ossec-client role on ansible server and other additional
    servers that have been configured earlier.  Ensure that
    roles/common_vars/vars/main.yaml clearly includes ansible server ip
    under ossec_client_ips.

*** Create DNS servers
    Create following servers:
    - public DNS
    - private DNS
    For each of these servers do the following:
    - Copy authorized keys from ansible server to these servers
    - Add server IP in ossec_client_ips in
      roles/common_vars/vars/main.yaml
    - Add server IPs in hosts file 
    
    Specifically for DNS servers look at variables in respective DNS
    server files.  Also ensure that roles/named_server/files has
    correct zone files for each zone with necessary zone_file_prefix.

*** Configure machines to use private DNS
    Edit roles/common_vars/vars/main.yaml and set private_dns_ips and
    private_dns_zone values appropriately.
    
*** Create rsyslog server
    Create a machine to setup rsyslog server.  Configure authorized
    keys from ansible server to this machine.  Add the machine IP to
    hosts file appropriately.  Add server IP to ossec_client_ips list.
    Also add the rsyslog server to IP binding in private and public DNS
    zone files.  Configure rsyslog server with common, ossec_client and
    rsyslog_server roles.

*** Configure machines to send logs to rsyslog server
    Add or uncomment role rsyslog_client on all servers except rsyslog_servers

*** Create machine for configuring reverseproxy
    Note that proper configuration of this and future steps depends
    upon availability of a few test labs.  Hence create a few lab
    VMs/containers as necessary before continuing.  Also update private
    DNS so that lab containers can be resolved using FQDN.

    Create a machine for reverseproxy.  Add its IP to private DNS.  Add
    its IP to ossec_client_IPs.  Configure reverseproxy with roles
    common, ossec_client, rsyslog_client and reverseproxy_server roles.
    Ensure that proxy_domains is set appropriately in
    common_vars/vars/main.yaml file.  Private DNS must be setup
    completely before reverseproxy is created.  
    
    Append following to /etc/httpd/conf/httpd.conf once after
    =NameVirtualHost *:80= line
    #+BEGIN_EXAMPLE
    <VirtualHost *:80>
     ServerAdmin contact@rekallsoftware.com
     DocumentRoot /var/www/html
     ServerName reverseproxy.virtual-labs.ac.in
     ServerAlias reverseproxy.vlabs.ac.in
     ErrorLog logs/reverseproxy.virtual-labs.ac.in-error_log
     CustomLog logs/reverseproxy.virtual-labs.ac.in-access_log common
    </VirtualHost>
    #+END_EXAMPLE
    and restart httpd.  This will help in seeing awstats statics on
    reverseproxy machine.
    
*** Create and configure router
    Create a router machine.  Add its ip in private zone files.  Add it
    to ossec_client_ips.  Ensure that values of following common_vars
    is set appropriately:
    - reverseproxy_ip
    - public_dns_ip
    - local_subnet
    - router_interface_ip
    Add authorized access from ansible server to router
    
    Update public DNS entries such that all requests resolve to router
    public IP except ansible for which there is a dedicated public IP.
    
** Bootstrapping the Base1/Base4 cluster
   This bootstrapping steps are done based on ansible playbooks of
   the systems-model repository. 
*** (OPTIONAL) Create VM
    - NOTE :: Creating a VM is an optional.You can go to next step for
              bootstrapping process.
    - To create VM, install the following packages
    #+BEGIN_SRC 
    yum install virt-manager libvirt libvirt-python libvirt-client qemu-kvm virt-viewer
    #+END_SRC
    - We can create VM in two ways
      1. Using *virt-manager* GUI (Applications-->System Tools-->Virtual Machine Manager)
	 Refer https://vlead.vlabs.ac.in/wiki/index.php/Creating_new_VM.
      2. Using *virsh-install* command http://wiki.centos.org/HowTos/KVM
         #+BEGIN_SRC 
         virt-install --os-variant=rhel6 --network bridge=br0 --disk /var/lib/libvirt/images/test-delet.img,size=20 --cdrom=~/iso/CentOS-6.5-x86_64-bin-DVD1.iso --vcpu=1 --ram=2048 --name=test-delete
         #+END_SRC
    - Use *virt-viewer* commnad to enter into the VM.
    - You can also access the VM over ssh 
    - Set proxy for all the nodes and labs if network uses any proxy
    - Find 3 available ips to assign to the vm, router and ansible machine.
    - Note ::  Private ips of the nodes on AWS and base{1,4} clusters
               are same except public IPs of router and ansible nodes.

*** Setup bridged network
**** Connect LAN interface to Bridge (br0) 
     Check your interfaces by running the command
     #+BEGIN_EXAMPLE 
      ifonfig -a
     #+END_EXAMPLE
     That gives you MAC address of interfaces(eth0, eth1, etc,.). Copy that.
     - Create *ifcfg-br0* file in =/etc/sysconfig/network-scripts/= and
       add the following fields

     #+BEGIN_EXAMPLE 
       DEVICE=br0
       BOOTPROTO=dhcp
       ONBOOT=yes
       TYPE=Bridge
       NM_CONTROLLED=no
     #+END_EXAMPLE

     - Then create another *ifcfg-eth0* file in
       =/etc/sysconfig/network-scripts/= and add the following fileds
     #+BEGIN_EXAMPLE 
      DEVICE=eth0
      HWADDR=<<Hardware Address of eth0 interface>>
      TYPE=Ethernet
      ONBOOT=yes
      NM_CONTROLLED=no
      BOOTPROTO=none
      BRIDGE=br0
     #+END_EXAMPLE

     Paste the copied MAC address of the interface in HWADDR field
     - Restart the network using

     #+BEGIN_EXAMPLE 
       service network restart
     #+END_EXAMPLE

     - Now you can see ip assigned to br0 and you will be able to get
       internet
**** Create bridge(br1) for private Network
     - Create *ifcfg-br1* file in =/etc/sysconfig/network-scripts/= and add
       the following fields
       #+BEGIN_EXAMPLE 
       DEVICE=br1
       TYPE=Bridge
       ONBOOT=yes
       NM_CONTROLLED=no
       BOOTPROTO=none
       #+END_EXAMPLE
     - Restart network again to see the created bridge(br1)

       #+BEGIN_EXAMPLE 
       service network restart
       #+END_EXAMPLE

     - Now you can see the bridges on your machine using *brctl show*
       command. That will give you the connections between interfaces.
     - We need to set proxy for all the containers if network uses any
       proxy.

     For more information go through the following links
    https://github.com/vlead/ovpl/blob/master/docs/bridge-setup.org

*** Setup base machine with OpenVZ etc. for ADS
    - Install git
#+BEGIN_EXAMPLE
yum -y install git
#+END_EXAMPLE
    - Setup base machine server with the following steps
      + Clone the repository from GitHub
	#+BEGIN_SRC 
	git clone https://github.com/vlead/setup-ovpl-centos.git
	#+END_SRC
      + Change directory to =setup-ovpl-centos/scripts/=
	#+BEGIN_SRC 
	cd setup-ovpl-centos/scripts/
	#+END_SRC
      + If your network uses a proxy server, then set proxy settings in config.sh.
        #+BEGIN_EXAMPLE
          export http_proxy="http://proxy.iiit.ac.in:8080"
          export https_proxy="http://proxy.iiit.ac.in:8080"
        #+END_EXAMPLE
      + Run =centos_prepare_ovpl.sh=.  
         #+BEGIN_SRC
         ./centos_prepare_ovpl.sh 
         #+END_SRC 
      + Running above script will install dependencies for openvz and
        for ADS server setup. It also downloads customized ubuntu
        template used for deploying labs from
        http://community.virtual-labs.ac.in/downloads/.
      + After this reboot the machine and choose openvz kernel from boot menu.
      + Perform the steps mentioned in the below link to make sure
        openvz is configured properly.
        https://www.sbarjatiya.com/notes_wiki/index.php/Installing_openVZ_on_Cent_OS
      + Download the latest centos template from below link and place at =/vz/template/cache=
        http://download.openvz.org/template/precreated/centos-6-x86_64.tar.gz
**** Troubleshooting VM OpenVZ issues
     In case the setup is performed in a VM and VM is unstable such
     that containers fail to stop and VM fails to reboot.  Then
     following changes may not help:
     - Reducing OpenVZ kernel version (downgrading)
     - Removing tboot
     - Update VM OS to latest packages
     Most probably issue would be with container template.  Download
     fresh template directly from openvz.org mirrors. 
*** Create Router 
    - Create router node as follows
      machine
      #+BEGIN_EXAMPLE 
      vzctl create 1001 --ostemplate centos-6-x86_64 --hostname router.vlabs.ac.in
      #+END_EXAMPLE 

    - Configure the network on the router, it needs to be connected
      with two bridges br0 and br1 in order to get internet connection
      and to setup private network via this router. Run the following
      commands.
       #+BEGIN_EXAMPLE 
       vzctl set 1001 --netif_add eth0,,,,br0  --save 
       vzctl set 1001 --netif_add eth1,,,,br1 --save
       vzctl set 1001 --onboot yes --save
      #+END_EXAMPLE

      It means, the container is created with two interfaces eth0 and
      eth1. eth0 of the router is connected to br0 of host machine and
      eth1 of the router is connected to br1.
    - Start the router
      #+BEGIN_EXAMPLE 
      vzctl start 1001
      #+END_EXAMPLE
    - Enter into the router
      #+BEGIN_EXAMPLE 
      vzctl enter 1001
      #+END_EXAMPLE
   - Set root passwd, and make note of it in some file.
#+BEGIN_EXAMPLE
passwd
#+END_EXAMPLE
**** Inside the Router 
     Now you may not get internet to the router. To get that do the
     following steps.

    - We need two IPs to setup router machine one is for accessing
      labs and servers over http, https and dns and another is for
      private network ip. This private ip is for gateway to all the
      nodes and lab containers.
    - Find one available ip in your network using ping command, to
      assign it to the router eth0 interface. This ip will become
      public ip for router.
    
     Create *ifcfg-eth0* file in =/etc/sysconfig/network-scripts/= and
     add the following fields. In place of HWADDR add MAC address of
     eth0 interface.
     #+BEGIN_EXAMPLE 
     DEVICE=eth0
     TYPE=Ethernet
     HWADDR=<MAC address of the eth0 interface>
     BOOTPROTO=static
     ONBOOT=yes
     NM_CONTROLLED=no
     IPADDR=<ip-address>
     NETMASK=<netmask>
     GATEWAY=<gateway>
     DNS1=<external-dns1>
     DNS2=<external-dns2>
     #+END_EXAMPLE

     Create another =ifcfg-eth1= file in
     =/etc/sysconfig/network-scripts/=.  This one is actually for
     creating private network. It acts like a Gateway to all other
     containers
 
     #+BEGIN_EXAMPLE 
     DEVICE=eth1
     HWADDR=<<Hardware address of eth1 interface>>
     BOOTPROTO=static
     ONBOOT=yes
     NM_CONTROLLED=no
     IPADDR=10.100.1.1
     NETMASK=255.255.252.0
     #+END_EXAMPLE
    - Restart the network 
      #+BEGIN_EXAMPLE
          service network restart
      #+END_EXAMPLE
    - NOTE ::  Check if you are able to access internet. If not then the
      gateway might be set to default value, not the one we mentioned
      in config file. Comment =networking= and =gateway= parameter in
      =/etc/sysconfig/network= file as follows, and restart the network.
#+BEGIN_EXAMPLE
#NETWORKING="yes"
#GATEWAYDEV="venet0"
NETWORKING_IPV6="yes"
IPV6_DEFAULTDEV="venet0"
HOSTNAME="router.{cluster}.vlabs.ac.in"
#+END_EXAMPLE

     If this approach does not work then download the fresh centos
     template from openvz and create router again.

**** Add  NAT rule and Edit =/etc/sysctl.conf= file
     Write a temporary NATing rule to get internet in private network
     containers and This rule will be used to run playbooks of private
     and public dns nodes and router nodes smoothly.
     #+BEGIN_EXAMPLE 
     iptables -t nat -A POSTROUTING ! -d 10.100.0.0/22 -o eth0 -j SNAT --to-source <router_public_ip>
     iptables-save > /etc/sysconfig/iptables
     #+END_EXAMPLE
      Edit/open the file =/etc/sysctl.conf= and modify the following line
     #+BEGIN_EXAMPLE 
     net.ipv4.ip_forward = 1
     #+END_EXAMPLE
     - Check the link for above things done for router
      https://openvz.org/Using_NAT_for_container_with_private_IPs
     - Restart the router container in order to enable ip forwarding
#+BEGIN_EXAMPLE
       reboot
#+END_EXAMPLE

*** Create Config-server node
**** Setup container
   - Create a machine.

     #+BEGIN_EXAMPLE
     vzctl create 1002 --hostname ansible.[{cluster-name}.]vlabs.ac.in --ostemplate centos-6-x86_64
     #+END_EXAMPLE

   - We need two IPs to setup ansible machine one is for public access
     over ssh and another is for private network ip. Private ip is for
     configuring other nodes as well as itself and internal
     connections.

     #+BEGIN_EXAMPLE
     vzctl set 1002 --netif_add eth0,,,,br0 --save
     vzctl set 1002 --netif_add eth0,,,,br1 --save
     vzctl set 1002 --onboot yes --save
     #+END_EXAMPLE

   - To configure the network do the same steps as followed for router
     node [[Inside the Router]], set IPADDR and MAC addresses of the
     interfaces appropriately. Set private IP as 10.100.1.2 for
     ansible container.

   - Start and enter inside the container.
   - Set root passwd
     #+BEGIN_EXAMPLE
       passwd
     #+END_EXAMPLE
   - Create a 'vlead' user.
     #+BEGIN_EXAMPLE
     adduser vlead 
     passwd vlead
     su - vlead
     ssh-keygen -t rsa
     #+END_EXAMPLE
  
   - Enable ssh access from localhost to localhost and router using
     key-based authentication

     #+BEGIN_EXAMPLE
     ssh-copy-id root@localhost  #from vlead user
     ssh-copy-id root@10.100.1.2  #from vlead user
     ssh-copy-id root@10.100.1.1  #from vlead user
     #+END_EXAMPLE
   - Note :: If the root user's ssh keys were copied earlier then the
     root user's private and public key can be placed inside the vlead
     user's .ssh directory, so that from vlead user we can login to other
     machine roots account.
   - Setup rpmfusion-free, rpmfusion-nonfree, rpmforge and
     epel-release repos on the system. Use the following command to
     install ansible
     #+BEGIN_EXAMPLE 
     yum -y install epel-release  #from root
     yum -y install ansible       #from root
     #+END_EXAMPLE
**** Install openssh if not installed
     By default some distro comes with openssh. If the package is not
     installed, install it using
     #+BEGIN_EXAMPLE 
     yum install openssh -y
     #+END_EXAMPLE
     and restart the ssh service 
     #+BEGIN_SRC 
     service sshd start
     chkconfig sshd on
     #+END_SRC
**** Version control setup
    - Setup version control on ansible machine using:

     #+BEGIN_EXAMPLE
     yum install git -y
     #+END_EXAMPLE

     In this node, git command is used to pull the updates from master
     branch of the systems model. From this node we do not push the
     updated files/folders to systems-model repository.

     Create .ssh/config file with following content, make sure
     permissions of the file are set to =400=. 
#+BEGIN_EXAMPLE
GSSAPIAuthentication no
#+END_EXAMPLE

    - If machine does not have direct internet access then you may
      need to add following lines also in .ssh/config file.
#+BEGIN_EXAMPLE
Host bitbucket.org
  HostName altssh.bitbucket.org
  Port 443
  User git
#+END_EXAMPLE

    - Copy ssh public keys of vlead user to systems-model repository
      in bitbucket.  Then clone the systems-model repository inside 'vlead'
      user home directory.
#+BEGIN_EXAMPLE
       git clone
       git@bitbucket.org:vlead/systems-model.git     #from vlead user git
#+END_EXAMPLE
     - checkout develop or other appropriate branch.
#+BEGIN_EXAMPLE
git checkout <branch-name>
#+END_EXAMPLE
     
**** Setup Emacs and org-mode-8
     - Setup emacs using:

     #+BEGIN_EXAMPLE
     yum -y install emacs
     #+END_EXAMPLE
     
     - Setup org-mode using (for VLEAD user):

     #+BEGIN_EXAMPLE
     mkdir -p ~/emacs/lisp   #as vlead user
     cd emacs/lisp
     wget http://orgmode.org/org-8.2.10.tar.gz
     tar zxvf org-8.2.10.tar.gz
     #+END_EXAMPLE

**** Run make and check syntax of site.yaml
     - Change directory to cloned repository systems-model and change
       the values of following variables in makefile
       + *ROUTER_IP* - Router's public IP.
       + *CONFIG_SERVER* - Ansible/Config server'e public IP.
       + *CLUSTER* - Name of the cluster
       + *SMTP_SMART_HOST* - Domain name of mail server/SMTP server.
      - And then run make 
      #+BEGIN_SRC 
      cd ~/systems-model/
      make
      #+END_SRC

    - Create hosts =hosts= file as follows and place it at
      =build/{cluster}-code/=
#+BEGIN_SRC txt :tangle hosts :eval no
[ansible_server]
#multiple-ok
#10.100.1.2 ansible_connection=local
10.100.1.2

[ossec_server]
#Only one allowed
10.100.1.3

[public_dns]
#multiple ok
10.100.1.6

[private_dns]
#multiple ok
10.100.1.5

[rsyslog_server]
#multiple ok
10.100.1.4

[reverseproxy_server]
#multiple ok
10.100.1.7

[router]
10.100.1.1

[nagios_server]
10.100.1.8

[rsnapshot_server]
10.100.1.10
#+END_SRC

    - Check syntax of ansible playbooks.

      #+BEGIN_EXAMPLE
      cd ~/systems-model/build/{cluster}
      ansible-playbook -i {cluster} --list-tasks --syntax-check site.yaml
      #+END_EXAMPLL

*** Create DNS servers and Configure
    - Create following servers:
      + public DNS
      + private DNS  
	#+BEGIN_SRC
        vzctl create 1005 --ostemplate centos-6-x86_64 --hostname privatedns.[{cluster}].vlabs.ac.in
        vzctl set 1005  --netif_add eth1,,,,br1 --save
        vzctl create 1006 --ostemplate centos-6-x86_64 --hostname publicdns.[{cluster}].vlabs.ac.in
        vzctl set 1006  --netif_add eth1,,,,br1 --save
        vzctl set 1005 --onboot yes --save
        vzctl set 1006 --onboot yes --save
	#+END_SRC
    - Start both containers and set root paaswd.
    - For each of the above servers do the following:
      + Configure the network-interface in
        =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following
        fields
        #+BEGIN_EXAMPLE 
        DEVICE=eth1
        HWADDR=<<Hardware address of eth1 interface>>
        BOOTPROTO=static
        ONBOOT=yes
        NM_CONTROLLED=no
        IPADDR=10.100.1.5/6(private/public)
        GATEWAY=10.100.1.1
        NETMASK=255.255.252.0
        #+END_EXAMPLE
      + Restart the network of dns nodes
        #+BEGIN_SRC 
        service network restart
        #+END_SRC
      + Enable ssh access from ansible using key-based authentication
        (=ssh-copy-id= from vlead user)
    - We need working DNS in new containers.  If working DNS are
      obtained due to inherit feature of OpenVZ then it is fine.

      Otherwise we should use temporary external dns server's IP
      other than cluster's dns server to configure these nodes using
      ansible playbooks.  

      For example edit /etc/resolv.conf file as follows:
      #+BEGIN_EXAMPLE
      nameserver 4.2.2.2
      nameserver 8.8.8.8
      #+END_EXAMPLE
    - Changes which we need to do in ansible machine as vlead user.
      Edit file =roles/common_vars/vars/main.yaml= are as follows
      + Ensure =ansible_server_ips= has 10.100.1.2
      + Set appropriate value for =admin_email_address=
      + Set =smtp_smart_host= appropriately, within iiith we can use 'smtp.admin.iiit.ac.in'
      + Comment =private_dns_ips: 10.100.1.5= , =private_dns_zone:
        base{1,4}.vlabs.ac.in= and uncomment =private-dns-ips: none=
        and =private-dns-zone: none=
      + =public_dns_ip= should be 10.100.1.6 
      + =local_subnet= should be 10.100.0.0/22
      + =router_interface_ip= should be public IP of router
      + =router_external_interface= should be eth0
      + =router_internal_ip= should be 10.100.1.1
      + =forwarders= should be '"{ }"' without single quotes, if you
        have direct internet access.
      + Change the value of =public-zone-address= variable (router's
        Public IP).
      + Set =is_amazon= based on whether cluster is being setup on AWS
        or not.  
      + Set appropriate value of prefix, eg "bstest.".  Note that
        prefix value should end with a dot(".").
      + =public_dns_entries= variable value with router and config-server
        public ips.  ansible IP would be used only once.  All other
        IPs should be replaced with router IPs.

	Values for router, ansible, private-dns and public-dns must be
        present.  All other values are optional at this point.
      + =private_dns_entries= variable also needs to be correct.
        Values for router, ansible, private-dns and public-dns must be
        present.  All other values are optional at this point.
    - Make sure that the hosts file with the ips of nodes and with
      their names according to name of the server's
      playbook. [private_dns] should be 10.100.1.5.  [public_dns]
      should be 10.100.1.6
    - Comment =rsnapshot_client= and =ossec_client= roles in
      private-dns.yaml and public-dns.yaml


    - NOTE :: Add =environment: proxy_env= in tasks/main.yaml file of
      the roles wherever a package is installed. This means package is
      installed using the proxy environment, which is defined in
      common_vars/main.yaml file.
#+BEGIN_EXAMPLE
- name: Installing all nagios plugins
  yum: name=nagios-plugins-all state=installed
  environment: proxy_env
#+END_EXAMPLE
    - Run *public_dns.yaml* and *private_dns.yaml* play books on
      config-server.
      #+BEGIN_SRC 
      ansible-playbook -i hosts public-dns.yaml
      #+END_SRC
      #+BEGIN_SRC 
      ansible-playbook -i hosts private-dns.yaml
      #+END_SRC
    - Once the nodes are configured using playbooks, test whether the
      dns nodes are functioning properly or not.  Try following
      commands:
      #+BEGIN_EXAMPLE
      nslookup www.google.co.in 10.100.1.5
      nslookup router.[{cluster}.]vlabs.ac.in 10.100.1.6
      nslookup router.[{cluster}.]vlabs.ac.in 10.100.1.5
      #+END_EXAMPLE

      If this does not work then first try to run router playbook to
      setup the router node, and then try these commands again.
      
*** Configure Router 
    - In order to apply proper firewall rules for our cluster we need
      to configure router after dns servers are configured.
    - Comment ossec_client and rsnapshot_client roles in router.yaml
    - Enable ssh acces from config-server node
      #+BEGIN_SRC 
      ansible-playbook -i hosts router.yaml
      #+END_SRC
    - NOTE :: Reason to run router.yaml play book here is: We added
              NATing rule as temporary firewall rule. So in common
              role there is task which restarts iptables service,
              temporary rule will be no more. 
	      
	      If we set POSTROUTING firewall rule as permanent rule
              then no need to run router.yaml file after dns servers
              are configured. 
    - Test router by doing
      #+BEGIN_EXAMPLE
      nslookup router.[{cluster}.].vlabs.ac.in <router-public-ip>
      #+END_EXAMPLE

      This query should resolve to router public IP itself.  This test
      did not work when tested in local vlead desktop machine.
*** Setup ossec server
    - Create ossec server.
      #+BEGIN_SRC 
      vzctl create 1003 --ostemplate centos-6-x86_64 --hostname ossec.[{cluster}.]vlabs.ac.in
      vzctl set 1003 --netif_add eth1,,,,br1 --save
      vzctl set 1003 --onboot yes --save
      #+END_SRC
    - Configure the network-interface in
      =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following
      fields
        #+BEGIN_EXAMPLE 
        DEVICE=eth1
        HWADDR=<<Hardware address of eth1 interface>>
        BOOTPROTO=static
        ONBOOT=yes
        NM_CONTROLLED=no
        IPADDR=10.100.1.3
        NETMASK=255.255.252.0
        #+END_EXAMPLE
    - Restart the network 
      #+BEGIN_SRC 
      service network restart
      #+END_SRC
    - Set root password and enable ssh access from ansible using
      key-based authentication
*** Create rsyslog server
    - Create a machine to setup rsyslog server.  
      #+BEGIN_SRC 
      vzctl create 1004 --ostemplate centos-6-x86_64 --hostname rsyslog.[{cluster}.].vlabs.ac.in
      vzctl set 1004 --netif_add eth1,,,,br1 --save
      vzctl set 1004 --onboot yes --save
      #+END_SRC
    - Configure the network-interface in
      =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following
      fields
        #+BEGIN_EXAMPLE 
        DEVICE=eth1
        HWADDR=<<Hardware address of eth1 interface>>
        BOOTPROTO=static
        ONBOOT=yes
        NM_CONTROLLED=no
        IPADDR=10.100.1.4
        NETMASK=255.255.252.0
        #+END_EXAMPLE
    - Restart the network 
      #+BEGIN_SRC 
      service network restart
      #+END_SRC
    - Set root password and enable ssh access from ansible using
      key-based authentication.
*** Create Reverse-proxy server
    - Create a machine for reverseproxy.  
      #+BEGIN_SRC 
      vzctl create 1007 --ostemplate centos-6-x86_64 --hostname reverseproxy.[{cluster}.].vlabs.ac.in
      vzctl set 1007 --netif_add eth1,,,,br1 --save
      vzctl set 1007 --onboot yes --save
      #+END_SRC
    - Configure the network-interface in
      =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following
      fields
      #+BEGIN_EXAMPLE 
      DEVICE=eth1
      HWADDR=<<Hardware address of eth1 interface>>
      BOOTPROTO=static
      ONBOOT=yes
      NM_CONTROLLED=no
      IPADDR=10.100.1.7
      NETMASK=255.255.252.0
      #+END_EXAMPLE
    - Restart the network 
      #+BEGIN_SRC 
      service network restart
      #+END_SRC
    - Set root password and enable ssh access from ansible using
      key-based authentication
    - We need to add ssl certificate to reverseproxy server manually
      before running reverseproxy_server.yaml play book from ansible
      machine.
    - Private DNS must be setup completely before reverseproxy is
      created.
      
*** Create Rsnapshot server
    - Rnsapshot server is for taking backup of necessary files and
      folders from required nodes of the cluster.
    - Create a container for rsnapshot server.
      #+BEGIN_SRC 
      vzctl create 1010 --ostemplate centos-6-x86_64 --hostname rsnapshot.[{cluster}.].vlabs.ac.in
      vzctl set 1010 --netif_add eth1,,,,br1 --save
      vzctl set 1010 --onboot yes --save
      #+END_SRC
    - Configure the network-interface in
      =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following
      fields
      #+BEGIN_EXAMPLE 
      DEVICE=eth1
      HWADDR=<<Hardware address of eth1 interface>>
      BOOTPROTO=static
      ONBOOT=yes
      NM_CONTROLLED=no
      IPADDR=10.100.1.10
      NETMASK=255.255.252.0
      #+END_EXAMPLE
    - Restart the network 
      #+BEGIN_SRC 
      service network restart
      #+END_SRC
    - set root password and enable ssh access from ansible using
      key-based authentication

*** Monitoring Server (Nagios)
    - Create nagios server to monitor services from all the nodes in
      the cluster
      #+BEGIN_SRC 
      vzctl create 1008 --ostemplate centos-6-x86_64 --hostname nagios.[{cluster}.].vlabs.ac.in
      vzctl set 1008 --netif_add eth1,,,,br1 --save
      vzctl set 1008 --onboot yes --save
      #+END_SRC
    - Configure the network-interface in
      =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following
      fields
      #+BEGIN_EXAMPLE 
      DEVICE=eth1
      HWADDR=<<Hardware address of eth1 interface>>
      BOOTPROTO=static
      ONBOOT=yes
      NM_CONTROLLED=no
      IPADDR=10.100.1.8
      NETMASK=255.255.252.0
      #+END_EXAMPLE
    - Restart the network 
      #+BEGIN_SRC 
      service network restart
      #+END_SRC
    - Set root password and enable ssh access from ansible using
      key-based authentication
     
*** Create ADS server
    - Create a container for ADS server. Mongod service requires container to be of size >= 10GB.
      vzctl create 1009 --ostemplate centos-6-x86_64 --hostname ads.[{cluster}.].vlabs.ac.in --diskspace 10G:10G
      #+BEGIN_SRC 
      vzctl set 1009 --netif_add eth1,,,,br1 --save
      vzctl set 1009 --onboot yes --save
      #+END_SRC
    - Configure the network-interface in
      =/etc/sysconfig/network-scripts/ifcfg-eth1= with the following
      fields
      #+BEGIN_EXAMPLE 
      DEVICE=eth1
      HWADDR=<<Hardware address of eth1 interface>>
      BOOTPROTO=static
      ONBOOT=yes
      NM_CONTROLLED=no
      IPADDR=10.100.1.9
      NETMASK=255.255.252.0
      #+END_EXAMPLE
    - Restart the network 
      #+BEGIN_SRC 
      service network restart
      #+END_SRC
    - Set root passwd and enable ssh access from ansible using
      key-based authentication
    - Install git, (set proxy if you are in a proxy environment)
      #+BEGIN_SRC 
      route add default gw 10.100.1.1
      echo "nameserver 10.4.12.160" > /etc/resolv.conf
      yum -y install git
      #+END_SRC
    - Generate root users ssh-keys.  Setup trust based ssh from ads
      container root user to Base machine.
#+BEGIN_EXAMPLE
ssh-keygen -t rsa
ssh-copy-id root@<base-machine-ip>
#+END_EXAMPLE
    - Clone *ovpl* 
      #+BEGIN_SRC 
      git clone https://github.com/vlead/ovpl.git
      #+END_SRC
    - Do following steps
      #+BEGIN_SRC 
      cd ovpl/config
      cp sample_config.json config.json
      cp sample_authorized_users.py authorized_users.py
      cd adapters
      cp sample_centos_bridged_config.py centos_bridged_config.py
      cp sample_base_config.py base_config.py
      cd ..
      #+END_SRC
    - Edit config.json as follows:
      + COOKIE_SECRET :: Change to some other random string
      + APP_URL :: Update app url to include FQDN
      + ADAPTER_TO_USE :: {"POOLID" : 1, "ADAPTERID" : 3 } 
        - In case of other types of setup use appropriate poolid or
          adapter Id.
      + LOGSERVER_CONFIGURATION / SERVER_IP :: 10.100.1.9
    - Edit authorized_users.py as follows:
      + Put bunch of gmail IDs separated by , as shown in the example
        config.  While deploying labs Gmail OpenID login will be
        used.  Also the given gmail ID may have to be associated with
        a persona account.
    - Edit adapters/centos_bridged_config.py as follows:
      + SUBNET_BRIDGE :: "br1"
    - Edit adapters/base_config.py as follows:
      + ADS_ON_CONTAINER = True
      + SUBNET = ["10.100.2.0/24"]
      + BASE_IP_ADDRESS = "root@10.4.14.202"  
        + Replace 10.4.14.202 with correct base machine (ie VM) IP
      + ADS_SERVER_VM_ID = "1009"
        + CTID of ADS container.  This is used while copying cloned
          labs and OVPL code from ADS container to lab container.
          Note that git clone of lab happens within ADS container and
          not directly in lab container. 
      + HOST_NAME = "{cluster}.vlabs.ac.in"
        + Domain name of lab-id
    - Again clone setup-ovpl-centos using:
#+BEGIN_EXAMPLE
      git clone https://github.com/vlead/setup-ovpl-centos.git
#+END_EXAMPLE
      - Install dependencies as follows
#+BEGIN_EXAMPLE
      cd setup-ovpl-centos/scripts
      vim config.sh #set proxyi settings here if network uses any proxy
      vim centos_prepare_ovpl.sh  #Comment OpenVZ installation lines 52 to 58
      ./centos_prepare_ovpl.sh    # Relax for 10 to 15 minutes or Go for a TEA
#+END_EXAMPLE 
    - Run services using
      #+BEGIN_EXAMPLE
      yum -y install python-setuptools
      cd ~/ovpl
      python setup.py install
      ./manage_services.sh start
      #+END_EXAMPLE
*** Run site.yaml on config-server
    - Make sure all necessary changes are done in
      =roles/common_vars/vars/main.yaml= appropriately.
    - Make sure that the hosts file is correct with the ips of nodes
      and with their names according to the name of the server's
      playbook .
    - If you are in a proxy environment then make sure you have
      enabled proxy environment, in the installation tasks.
    - Check management_ips in common_vars role
    - Uncomment =private_dns_ips: 10.100.1.5= , =private-dns-zone:
      {cluster}.vlabs.ac.in= and comment =private-dns-ips: none=,
      =private-dns-zone: none= in common_vars role.
    - Add servers internal ips to =ossec_client_ips= variable in
      common_vars role.
    - Comment =ossec_client= in all servers playbook.
    - Comment =ossec_server.yaml= in =site.yaml=
    - Check syntax before running the site.yaml
      #+BEGIN_EXAMPLE
      ansible-playbook -i hosts --list-tasks --syntax-check site.yaml
      #+END_EXAMPLE
    - Run site.yaml file. If you get any error try to run it again,
      the error may resolve.
      #+BEGIN_SRC 
      ansible-playbook -i hosts site.yaml
      #+END_SRC

*** Host labs using ads service
    - Deploy a few labs using ads service to test the whole
      setup. Open http://ads.{cluster}.vlabs.ac.in:8080 and provide
      required fields.
    - After deploying the lab update the entries in common_vars of
      follwoing variables appropriately.
#+begin_example
      proxy_domains:
        - {domain: "{lab-id}.{cluster}.virtual-labs.ac.in", alias: {lab-id}.{cluster}.vlabs.ac.in }
      awstats_domains:
        - {lab-id}.{cluster}.virtual-labs.ac.in
      public_dns_entries:
        - {hostname: {lab-id}, ip: <router-public-ip> }
      private_dns_entries:
        - {hostname: {lab-id}, ip: <lab-ip> }
#+end_example
    - Then run ansible-playbooks of three roles as follows:
#+BEGIN_EXAMPLE
ansible-playbook -i hosts reverseproxy_server.yaml
ansible-playbook -i hosts public-dns.yaml
ansible-playbook -i hosts private-dns.yaml
#+END_EXAMPLE
     - Check whether the lab has been deployed or not using provided
       lab url.

* Design of the cluster model
  The diagram below depicts the architecture of the nodes in the
  cluster model.  This architecture is same for all the three
  deployments (base1, base4 and AWS).  The diagram shows the
  network-setup of the cluster and connectivity between a few key
  nodes.

   #+CAPTION:  Cluster Network Diagram
   #+LABEL:  fig-cluster-network-diagram
   [[./diagrams/overall-cluster-network-diagram.png]]
   
  In this diagram IP1 and IP2 refer to the only two public IP's being
  used by the system.  IP1 is the public IP assigned to router and IP2
  is the public IP assigned to ansible.  Normal virtual-lab users only
  see and contact router.  VLEAD team uses ansible to manage the
  cluster.  In near future ADS interface will also be accessible via
  router, for all virtual-labs developers, perhaps through a different
  port. \\
  This diagram focuses on depicting the important connections in the
  network. There are several other connections which are not shown.
  For example, ansible is connected to every node in the cluster.  All
  the connections are not shown in the diagram. Also there are some
  nodes which are not shown, for example the rsnapshot server, rsyslog
  server, ossec server are not shown. But these nodes are configured
  and are being used in the cluster but are not shown to keep the
  diagram simple.
   
  /In near future an ADS interface will also be accessible via/
  /router, for all the virtual-labs developers, perhaps through a/
  /different port./

* Nodes in the cluster
  There are several nodes in the cluster which are all common to the
  various clusters. The list of nodes is as below:
  
  - [[./rsyslog-server.org][Rsyslog server]]
  - [[./private-dns.org][Private DNS]]
  - [[./public-dns.org][Public DNS]]
  - [[./rp-awstats.org][Reverse Proxy]]
  - [[./nagios-server.org][Nagios server]]
  - [[./router.org][Router]]
  - [[./rsnapshot-server.org][Backup server]]
  - [[./ossec-server.org][OSSEC server]] 
  - [[./lab_rsnapshot_server.org][Lab Backup Server]]
  - [[./lab-role.org][Lab node]]

  After the bootstrapping process the nodes in the cluster can be
  brought up using the script below. It is an ansible script which
  will bring up all the nodes one after another with the required
  roles.

#+BEGIN_SRC YAML :tangle site.yaml 
---
#Complete site configuration file
#One yaml file for each server is included here
#Server yaml file matches server FQDN


- include: rsnapshot_server.yaml

- include: ossec_server.yaml

- include: config_server.yaml

- include: router.yaml

- include: public_dns.yaml

- include: private_dns.yaml

- include: rsyslog_server.yaml

- include: reverseproxy_server.yaml

- include: nagios_server.yaml

#- include: ads_server.yaml

- include: lab_rsnapshot_server.yaml

#+END_SRC 
* AWS compliance
  AWS provides a list of security best practices which should be
  followed. These practices help the AWS client take safety
  precautions based on the various features Amazon provides. Using
  these a compliance test of the AWS cluster was done. The following
  file describes the same.

  [[./vlead-compliance-with-aws-security-best-practicses.org][Vlead compliance with aws security best practicses]]

* Known Issues
  - A separate repository for systems related issues which includes
    the cluster and its implementation can be viewed [[https://bitbucket.org/vlead/systems/issues?status%3Dnew&status%3Dopen][here]].
* Appendix
** Version control
   The cluster model would involve timely changes, be it in the
   configuration of the nodes or the architecture of the
   cluster. Version control provides a mechanism which would keep
   track of all such changes. Along with tracking changes, it also
   provides the facility to revert back to a previous state of the
   system. The version control system used in this model is "git". To
   know more about git [[https://git-scm.com/][click here]].

** OpenVZ
   OpenVZ is a simple open source kernel level virtualization.  It is
   used to create several isolated containers (Virtual Environments)
   on the same physical resources. Details on why OpenVZ is used can
   be found [[https://openvz.org/Use_cases][here]]. 

   Base1 and Base4 clusters are built using such containers. To know
   more about how to setup containers etc. [[https://openvz.org/User_Guide/Operations_on_Containers][click here]].

** AWS
   AWS stands for Amazon Web Services. It is a proprietary cloud
   service provider. Maintenance and allocation of resources are taken
   care of by the service provider (Amazon). 

   AWS cluster is setup using the Amazon web services. To know more
   about AWS service please read the following :
   + [[https://en.wikipedia.org/wiki/Amazon_Web_Services][Amazon Web services]]
   + [[http://aws.amazon.com/free/][AWS Free Tier]]
   + [[http://aws.amazon.com/ec2/][Amazon EC2]]
   + [[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html][AWS EC2 user guide]]
     
** Networking
   Some basics topics in networking that are required to know are:
   + Bridged networks (using OpenVZ) [[[https://en.wikipedia.org/wiki/Bridging_%2528networking%2529][ Bridging]], [[https://openvz.org/Virtual_Ethernet_device][Bridging in OpenVZ]], [[https://openvz.org/Category:Networking][OpenVZ Networking]] ]
   + Concept of subnets [ [[https://en.wikipedia.org/wiki/Subnetwork][Subnetwork]] ] 
   + Basic knowledge of IP addressing [ [[https://en.wikipedia.org/wiki/IP_address][IP addresing]] ]

** Ansible
   Ansible is a configuration management tool which is used for
   configuring the cluster. It provides remote configuration of nodes
   over SSH. For more details view the following [[https://en.wikipedia.org/wiki/Ansible_%2528software%2529][Ansible]] , [[http://docs.ansible.com/][Ansible
   Documentation]].
* COMMENT Bootstrapping of a cluster deployment
  The cluster is a collection of several nodes.  Each of these nodes
  have a specific purpose that they serve.  To setup the entire
  cluster for the first time we need to setup each of the nodes
  individually.  This process is called the bootstrapping process.
  This process is tightly related to provisioning on corresponding
  cluster. 
** Provisioning vs bootstrapping
   *Provisioning* is a term used to describe the process for bringing up
   a system for setting up the node.\\
   *Bootstrapping* process uses this system to setup the
   nodes. Provisioning is a part of the bootstrapping process, hence
   they cannot be separated.\\
** Bootstrapping of Cluster
   All the nodes of the three deployments are same.  The main
   difference is in the provisioning.  The details of provisioning of
   these clusters is described in the following sections.

*** Provisioning for Base1 and Base4
    The 
# comment Above statement is incomplete
*** Provisioning for AWS
# comment This section needs to be filled    
* COMMENT Difference in the base1 and AWS cluster
  Most of the components of base1 and base4 clusters are same as AWS
  cluster.  The main difference is in the provisioning.  The details of
  provisioning of these clusters is described in this section.

** Provisioning of the base1 and base4 clusters
   *TODO*
** Provisioning of the AWS cluster
   *TODO*
